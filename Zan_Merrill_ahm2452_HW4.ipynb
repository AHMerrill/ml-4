{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MIS 382N: Advanced Machine Learning Assignment 4\n",
        "\n",
        "**Total points**: 55 pts\n",
        "\n",
        "**Due**: 11:59 PM CST, Tuesday, November 4th, 2025.\n",
        "\n",
        "**Submission**:\n",
        "1. Submit your **Jupyter Notebook via Canvas**, AND\n",
        "2. **Save your Jupyter Notebook to a PDF, and submit the PDF via Gradescope**.\n",
        "\n",
        "You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas and Gradescope. But be sure to include the name and UT EID for both students.\n",
        "\n",
        "Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "For questions involving mathematical derivations, you can write your answer on paper and then upload an image. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
      ],
      "metadata": {
        "id": "eoCxNT8Dnaeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name(s) and EID(s)**:\n",
        "\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "3Q7oUnKCqpms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bookmarks:\n",
        "\n",
        "Q1. <a href=#Q1>Dimensionality Reduction - tSNE</a>\n",
        "\n",
        "Q2. <a href=#Q2>Classification With a Loss Matrix</a>\n",
        "\n",
        "Q3. <a href=#Q3>Visualizing the Confusion Matrices, ROC Curves, and PR Curves</a>\n",
        "\n",
        "Q4. <a href=#Q4>Concepts about AU-ROC and AU-PRC</a>\n",
        "\n",
        "-------------------------"
      ],
      "metadata": {
        "id": "mw23xiTndyoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Dimensionality Reduction - tSNE** (15 pts) <a name='Q1' />\n",
        "\n",
        "In this problem, you will apply Principal Component Analysis (PCA) to the Superconductivity Dataset. More details about the dataset are available [here](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data#). The goal of this exercise is to characterize the structure of the input feature space.\n",
        "\n",
        "Let's start with the following steps to prepare the dataset:\n",
        "\n",
        "1. Load the dataset from the file `Q1_data.csv` into a pandas DataFrame named `df`.\n",
        "2. The dataset contains a column called `critical_temp`, representing the critical temperature of each superconductor. Please select the column `critical_temp` as `y`.\n",
        "**We will not use this column when fitting t-SNE. However, you may retain it later for visualization (e.g., coloring points by `y`).**\n",
        "3. Define the feature matrix: select all remaining columns (except `critical_temp`) from `df` as `X`.\n",
        "4. Standardize the features using [Standard Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler): Fit the scaler on `X`, and then transform `X` to obtain the standardized data. Apply scaling only to `X`, not to `y`.\n",
        "\n",
        "Note: After this step, `X` should contain 81 features.\n"
      ],
      "metadata": {
        "id": "EMeCRxL2B9KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "qHCkjK_4nI0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "import os, sys, re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"Q1_data.csv\")"
      ],
      "metadata": {
        "id": "5oQlBYvRnK6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"critical_temp\"]\n",
        "X = df.drop(columns=[\"critical_temp\"])\n",
        "\n",
        "scalar = StandardScaler()\n",
        "scalar.fit(X)\n",
        "X = scalar.transform(X)"
      ],
      "metadata": {
        "id": "-v4HeP2cnNYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1.** (5 points) Now apply T-SNE to the dataset. You are required to carry out the following tasks:\n",
        "\n",
        "1.   Initialize a t-SNE model with number of dimensions = 3, perplexity = 300, number of iterations = 300 and random state = 42.\n",
        "2.   Apply the t-SNE model to the data.\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "pDSpWdsGnICm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "### START CODE ###\n",
        "## Initialize t-SNE with: n_components=3, perplexity=300, n_iter=300, random_state=42\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "### START CODE ###\n",
        "## Apply t-SNE to the training dataset\n",
        "\n",
        "### END CODE ###"
      ],
      "metadata": {
        "id": "zMbHyXtfCGVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2**. (5 points) Sample 1000 random data points from the dataset, and show the 2D scatter plots of their first three t-SNE components.\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "2QUVR5hjCIQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE ###\n",
        "# Sample 1000 random data points from the dataset\n",
        "# Plot first two components (2D scatter)\n",
        "# Hint: Use X_tsne[:1000, 0] and X_tsne[:1000, 1] as x/y axes.\n",
        "# Color by y\n",
        "\n",
        "### END CODE ###"
      ],
      "metadata": {
        "id": "1NQydMUAwSK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE ###\n",
        "# Plot the first and the third components (2D scatter)\n",
        "\n",
        "### END CODE ###"
      ],
      "metadata": {
        "id": "mJekLD9vwT4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE ###\n",
        "# Plot the second and the third components (2D scatter)\n",
        "\n",
        "### END CODE ###"
      ],
      "metadata": {
        "id": "qJUrpChwweBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3**. (5 points) Now we will plot the PCA and t-SNE projections of the data and compare the plots side-by-side to see the difference in scatters created by the two methods. You can use first 1000 data points for this.\n",
        "\n",
        "**Answer**:"
      ],
      "metadata": {
        "id": "UU6rNKkRCPzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### START CODE ###\n",
        "## Obtain components from PCA\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "### START CODE ###\n",
        "## Obtain components from t-SNE\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "# Plot side-by-side\n",
        "plt.figure(figsize=(12, 5))  # Adjust the figure size as needed\n",
        "\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, select the first subplot\n",
        "plt.title('PCA')\n",
        "### START CODE ###\n",
        "## Left plot: scatter plot of PCA results for the first 1000 data points\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, select the second subplot\n",
        "plt.title('T-SNE')\n",
        "### START CODE ###\n",
        "## Right plot: scatter plot of t-SNE results for the first 1000 data points\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lXK8ut7DK8dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Classification With a Loss Matrix** (10 pts) <a name='Q2' />\n",
        "\n",
        "Consider a binary classification problem with the following loss matrix:\n",
        "$$\n",
        "   {\\begin{array}{ccccc}\n",
        "   & & \\text{Predicted class} & \\text{           } &\\\\\n",
        "   & & C1 & C2 & Reject\\\\\n",
        "   \\text{True class} & C1 & 0 & r & c  \\\\\n",
        "   & C2 & s & 0 & c \\\\\n",
        "  \\end{array} }\n",
        "$$\n",
        "\n",
        "where the cost of rejection is a constant, and the costs $r$ and $s$ are positive real numbers. Let $f(x)=P(C1|x)$.\n",
        "\n",
        "\n",
        "**Part 1.** (2 points) Show that the expected loss when $x$ is predicted as $C_1$ is a decreasing function of $f(x)$ while expected loss when $x$ is predicted as $C_2$ is a increasing function of $f(x)$.\n",
        "\n",
        "Hint: Express the expected losses with $f(x)$.\n",
        "\n",
        "**Answer:**\n"
      ],
      "metadata": {
        "id": "LPWsGQB0MnH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2.** (2 points) If $c=0$, show that the decision that minimizes the expected loss is to reject all instances of $x$.\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CbCU3daNwhJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3.** (3 points) Let $r=4$ and $s=3$, what is the minimum value of $c$ such that no instance of $x$ should be rejected under the optimal decision?\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "i7wW5y6Gws_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4** (3 points) Let $r=6$, $s=3$, and $c=1$. Determine the  ranges of $f(x)$ for which the optimal decision is C1, reject and C2 respectively.\n",
        "\n",
        "**Answer:**\n"
      ],
      "metadata": {
        "id": "eJ-QaXQYwzEx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jICIvpiIN4oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70J_XclV-Npj"
      },
      "source": [
        "**Q3. Visualizing the Confusion Matrices, ROC Curves, and PR Curves** (20 pts) <a name='Q3' />\n",
        "\n",
        "In this question, we will train a logistic regression classifier and a multi-layer perceptron classifier on a binary classification problem. Then, we will visualize the confusion matrices, ROC Curves, and PR Curves as well as evaluating the AU-ROC and AP of these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1_WjO7b-T1V"
      },
      "outputs": [],
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TENsxCBS-Wtk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "pd.set_option('future.no_silent_downcasting', True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataloading and Preprocessing**"
      ],
      "metadata": {
        "id": "WZPzLruUi_48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('customer_churn_telcom.csv', index_col = [0])"
      ],
      "metadata": {
        "id": "qvdyOBdsO0rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Op6SMz-Y_k"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "EvxfWn8cO-fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHq87DSg-aUT"
      },
      "outputs": [],
      "source": [
        "# Print out unique values of categorical columns\n",
        "def print_unique_col_values(df):\n",
        "    for column in df:\n",
        "        if df[column].dtypes=='object':\n",
        "            print(f'{column}: {df[column].unique()}')\n",
        "\n",
        "print_unique_col_values(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bPkT_pY-cNI"
      },
      "outputs": [],
      "source": [
        "# Replace values of 'no internet service' and 'no phone service' with the value  'No'\n",
        "df.replace('No internet service', 'No', inplace=True)\n",
        "df.replace('No phone service', 'No', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7vxmlGz_DXO"
      },
      "outputs": [],
      "source": [
        "# Change categorical columns with 2 categories to 0/1\n",
        "yes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',\n",
        "                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']\n",
        "\n",
        "for col in yes_no_columns:\n",
        "    df[col] = df[col].replace({'Yes': 1, 'No': 0})\n",
        "\n",
        "df['gender'] = df['gender'].replace({'Female': 1, 'Male': 0})\n",
        "\n",
        "for col in yes_no_columns + ['gender']:\n",
        "    print(f'{col}: {df[col].unique()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXvBsxtE_HER"
      },
      "outputs": [],
      "source": [
        "# One hot encoding for categorical columns with more than two categories\n",
        "df = pd.get_dummies(data = df, columns = ['InternetService','Contract','PaymentMethod'])\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "X = df.drop('Churn', axis='columns')\n",
        "y = df.Churn.astype(np.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
      ],
      "metadata": {
        "id": "UfATN4P9kEd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbHRPJLH_QFk"
      },
      "outputs": [],
      "source": [
        "# Standardize numerical columns\n",
        "cols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
        "X_val[cols_to_scale] = scaler.transform(X_val[cols_to_scale])\n",
        "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpA5ALBy_uqR"
      },
      "outputs": [],
      "source": [
        "# With samples corresponding to the positive class being very low, we can clearly see the imbalance in our data\n",
        "print('Churn occurences in the training set \\n')\n",
        "print(y_train.value_counts())\n",
        "print('\\n')\n",
        "print('Churn occurences throughout the data \\n')\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWDjCV45_z60"
      },
      "source": [
        "**Part 1.** (5 points) In this part, train a **logistic regression model** for the churn prediction problem. Then, evaluate its AU-ROC and AP on **train, validation, and test sets**. Finally, visualize its confusion matrices on the **validation and test set** side by side. Use ```random_state=42``` for training your logistic regression model.\n",
        "\n",
        "Helpful resources:\n",
        "1. [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/0.16/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "2. [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
        "3. [sklearn.metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW7EBFOWDZcZ"
      },
      "outputs": [],
      "source": [
        "# Initialize and fit logistic regression model\n",
        "### START CODE ###\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "\n",
        "# Evaluate AU-ROC and AP on train/validation/test sets\n",
        "### START CODE ###\n",
        "train_y_pred_score_lr = ...\n",
        "val_y_pred_score_lr = ...\n",
        "test_y_pred_score_lr = ...\n",
        "\n",
        "train_auroc_lr, val_auroc_lr, test_auroc_lr = ..., ..., ...\n",
        "train_ap_lr, val_ap_lr, test_ap_lr = ..., ..., ...\n",
        "### END CODE ###\n",
        "print(f\"Logistic Regression Train AU-ROC: {train_auroc_lr:.3f} Validation AU-ROC: {val_auroc_lr:.3f} Test AU-ROC: {test_auroc_lr:.3f}\")\n",
        "print(f\"Logistic Regression Train AP: {train_ap_lr:.3f} Validation AP: {val_ap_lr:.3f} Test AP: {test_ap_lr:.3f}\")\n",
        "\n",
        "# Visualize confusion matrices on validation and test sets\n",
        "### START CODE ###\n",
        "\n",
        "### END CODE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2a.** (3 points) In this part, we will train a **multi-layer perceptron** (MLP) for the churn prediction problem. For simplicity and consistency, use sklearn's ```MLPClassifier``` with ```random_state=42```. Before training the final MLP, first do hyper-parameter selection by validation average precision score.\n",
        "\n",
        "Hint: Try out different ```hidden_layer_sizes``` and ```learning_rate_init``` configurations for ```MLPClassifier```.\n",
        "\n",
        "Helpful resources:\n",
        "1. [sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
        "2. [sklearn.metrics.average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)\n",
        "3. [sklearn.model_selection.ParameterGrid](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html)\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "k7nXdgHxJi-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyper-parameter search space, train models with different hyper-parameters, and record the best hyper-parameters and validation AP\n",
        "### START CODE ###\n",
        "\n",
        "best_val_ap_mlp = ...\n",
        "best_hparams_mlp = ...\n",
        "\n",
        "\n",
        "### END CODE ###\n",
        "print(f\"Best Hyper-parameters: {best_hparams_mlp} Best Val Average Precision: {best_val_ap_mlp:.3f}\")\n"
      ],
      "metadata": {
        "id": "dcj_S5KkdC0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2b.** (2 points) Use your selected best hyper-parameters to train a final MLP. Then, evaluate its AU-ROC and AP on **train, validation, and test sets**. Finally, visualize its confusion matrices on the **validation and test set** side by side.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "hAkoh-UVLRRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybj7IyhDDcOx"
      },
      "outputs": [],
      "source": [
        "# Initialize and fit MLP\n",
        "### START CODE ###\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "# Evaluate AU-ROC and AP on train/validation/test sets\n",
        "### START CODE ###\n",
        "train_y_pred_score_mlp = ...\n",
        "val_y_pred_score_mlp = ...\n",
        "test_y_pred_score_mlp = ...\n",
        "\n",
        "train_auroc_mlp, val_auroc_mlp, test_auroc_mlp = ..., ..., ...\n",
        "train_ap_mlp, val_ap_mlp, test_ap_mlp = ..., ..., ...\n",
        "### END CODE ###\n",
        "\n",
        "print(f\"MLP Train AU-ROC: {train_auroc_mlp:.3f} Validation AU-ROC: {val_auroc_mlp:.3f} Test AU-ROC: {test_auroc_mlp:.3f}\")\n",
        "print(f\"MLP Train AP: {train_ap_mlp:.3f} Validation AP: {val_ap_mlp:.3f} Test AP: {test_ap_mlp:.3f}\")\n",
        "\n",
        "# Visualize confusion matrices on validation and test sets\n",
        "### START CODE ###\n",
        "\n",
        "### END CODE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weuN8JjiDTZ3"
      },
      "source": [
        "**Part 3.** (3 points) Plot the ROC curves of the trained logistic regression and MLP models on the validation and test sets side by side. The left plot is for validation set and the right plot is for test set. Each plot consists of two ROC curves, one of the logistic regression model and one of the MLP. Please also display the AU-ROC (or AUC) to the **3rd decimal place** in the plot as well.\n",
        "\n",
        "Helpful resources:\n",
        "1. [sklearn.metrics.RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html)\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMSK7BJDDWhT"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curves\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
        "\n",
        "### START CODE ###\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0woq3xlNDhXx"
      },
      "source": [
        "**Part 4.** (3 points) Plot the PR curves of the trained logistic regression and MLP models on the validation and test sets side by side. The left plot is for validation set and the right plot is for test set. Each plot consists of two PR curves, one of the logistic regression model and one of the MLP. Please also display the average precision (AP) to the **3rd decimal place** in the plot as well.\n",
        "\n",
        "Helpful resources:\n",
        "1. [sklearn.metrics.PrecisionRecallDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html)\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "### START CODE ###\n",
        "\n",
        "### END CODE ###\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eafQH2BgdFop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5.** (4 points) Briefly describe your takeaways from the plotted ROC and PR curves and how do the two models you trained compare.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "aWcRXS763z5M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcJkYPB2cfBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Concepts about AU-ROC and AU-PRC** (10 pts) <a name='Q4' />\n",
        "\n",
        "**Part 1.** (4 points) Consider a binary classification problem and a no-skill random classifier $f(x)$ that outputs a number randomly sampled from $U[0,1]$ to be the predicted probability of $x$ being positive. In other words, the returned probability from $f(x)$ actually does not depend on $x$ (hence no-skill). Why is the Area Under the Preciison-Recall Curve (AU-PRC) of $f(x)$ the prior probability of the positive class, i.e. $P(y = 1)$?\n",
        "\n",
        "HINT: Think about the definition of precision and recall, and how they relate to the decision threshold.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "px0E0_LntBRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2.** (3 points) Consider the previous binary classification problem and classifier. Why is the Area Under the Receiver Operating Characteristic Curve (AU-ROC) of $f(x)$ $0.5$?\n",
        "\n",
        "HINT: Think about the definition of true positive rate and false positive rate, and how they relate to the decision threshold.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "I1ZCCeON25Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3.** (3 points) Why is AP a better metric than AU-ROC under an imbalanced binary classification problem where the positive instances are extremely rare compared to negative instances?\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "v-z6TxWCuHLX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0lpiVJIPtH73"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
